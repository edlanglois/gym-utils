#!/usr/bin/env python
"""Run an RL algorithm on an OpenAI gym environment."""
import argparse
import importlib
import itertools
import os
import os.path
import tempfile
import time

import gym
from gym_utils import agents, cli


def parse_args():
    """Parse command-line arguments.

    Returns:
        An `argparse.Namespace` object containing the parsed arguments.
    """
    parser = argparse.ArgumentParser(description=__doc__.splitlines()[0]
                                     if __doc__ else None)
    parser.add_argument(
        '-e', '--env', default='CartPole-v0', help='Environment ID.')
    parser.add_argument(
        '-a',
        '--agent',
        action=cli.DictLookupAction,
        default='random',
        choices={
            'random': agents.RandomAgent,
            'dqn': agents.DeepQAgent,
        },
        help='Agent')
    parser.add_argument(
        '--max-train-steps',
        default=10000,
        type=int,
        metavar='N',
        help='Run training for at most %(metavar)s steps.')
    parser.add_argument(
        '--no-render',
        action='store_false',
        dest='render',
        help="Don't render the environment.")
    parser.add_argument(
        '-I',
        '--import',
        type=str,
        nargs='+',
        dest='import_',
        metavar='MODULE',
        help='Import the named modules before creating the environment.')

    parser.add_argument(
        '--log',
        action='store_true',
        help='If true, store logs about the run.')
    parser.add_argument(
        '--log-dir',
        default=os.path.join(tempfile.gettempdir(),
                             os.path.splitext(os.path.basename(__file__))[0]),
        help='Base logging directory. (default: %(default)s)')

    return parser.parse_args()


def main():
    args = parse_args()
    if args.import_:
        for module_name in args.import_:
            importlib.import_module(module_name)

    log_dir = os.path.join(args.log_dir, time.strftime('%Y.%m.%d-%H.%M.%S'))
    if args.log:
        print('Logging to:', log_dir)

    env = gym.make(args.env)
    env = args.agent.wrap_env(env)
    agent = args.agent(env)

    if args.log:
        train_env = gym.wrappers.Monitor(env, log_dir)
    else:
        train_env = env

    print('Training...')
    agent.train(max_timesteps=args.max_train_steps, env=train_env)

    if args.log:
        train_env.close()
        agent.save(log_dir)

    # Note: training can still happen here, it just no longer part of the
    # agent-directed training stage.
    print('Running...')
    observation = env.reset()
    for i in itertools.count():
        action = agent.act(observation)
        observation, reward, done, _ = env.step(action)
        agent.update(observation, reward, done)
        if args.render:
            env.render()
        if done:
            observation = env.reset()


if __name__ == '__main__':
    main()
